#!/usr/bin/env python3
"""
Service Mistral AI avec gestion robuste du rate limit
Niveau ENTREPRISE - S√©curit√© et √©thique AI
"""

import os
import time
import logging
import requests
from typing import Dict, List, Optional
import json

# Configuration du logging
logger = logging.getLogger(__name__)

class MistralAIService:
    """Service Mistral AI avec gestion robuste du rate limit"""
    
    def __init__(self):
        """Initialisation du service avec gestion intelligente du rate limit"""
        
        # Chargement automatique de la configuration
        try:
            from src.api.config import MISTRAL_API_KEY as config_key
            if config_key and config_key != "":
                self.api_key = config_key
                logger.info("‚úÖ Configuration Mistral AI charg√©e depuis config")
            else:
                self.api_key = os.getenv('MISTRAL_API_KEY')
                if not self.api_key:
                    logger.warning("‚ö†Ô∏è Cl√© Mistral AI non configur√©e (config ou env)")
                else:
                    logger.info("‚úÖ Configuration Mistral AI charg√©e depuis env")
        except ImportError:
            self.api_key = os.getenv('MISTRAL_API_KEY')
            if not self.api_key:
                logger.warning("‚ö†Ô∏è Cl√© Mistral AI non configur√©e (env uniquement)")
            else:
                logger.info("‚úÖ Configuration Mistral AI charg√©e depuis env")
        
        self.base_url = "https://api.mistral.ai/v1"
        self.model = "mistral-large-latest"  # Mod√®le le plus avanc√©
        
        # GESTION ROBUSTE DU RATE LIMIT SELON VOS RECOMMANDATIONS
        self.max_requests_per_minute = 10  # Limite conservatrice
        self.min_delay_between_requests = 0.2  # 200ms minimum entre requ√™tes
        self.request_timestamps = []
        self.last_request_time = 0
        self.rate_limit_reset_time = None
        self.consecutive_failures = 0
        self.max_consecutive_failures = 3
        self.backoff_multiplier = 2
        self.max_backoff_time = 60  # Maximum 60 secondes
        
        if self.api_key:
            logger.info("‚úÖ Service Mistral AI initialis√© avec cl√© API")
            logger.info(f"üîß Rate limit configur√©: {self.max_requests_per_minute} req/min")
            logger.info(f"‚è±Ô∏è D√©lai minimum entre requ√™tes: {self.min_delay_between_requests}s")
            logger.info(f"üîÑ Strat√©gie de retry avec backoff exponentiel activ√©e")
        else:
            logger.warning("‚ö†Ô∏è Service Mistral AI en mode fallback uniquement")
    
    def _check_rate_limit(self) -> bool:
        """V√©rification intelligente du rate limiting avec gestion des erreurs"""
        now = time.time()
        
        # Nettoyer les anciennes requ√™tes
        self.request_timestamps = [ts for ts in self.request_timestamps if now - ts < 60]
        
        # V√©rifier le d√©lai minimum entre requ√™tes (200ms)
        if self.last_request_time > 0:
            time_since_last = now - self.last_request_time
            if time_since_last < self.min_delay_between_requests:
                sleep_time = self.min_delay_between_requests - time_since_last
                logger.info(f"‚è±Ô∏è Attente de {sleep_time:.2f}s pour respecter le d√©lai minimum")
                time.sleep(sleep_time)
                now = time.time()  # Mettre √† jour le temps apr√®s l'attente
        
        # V√©rifier le nombre de requ√™tes par minute
        if len(self.request_timestamps) >= self.max_requests_per_minute:
            logger.warning(f"üö® Rate limit d√©pass√©: {len(self.request_timestamps)}/{self.max_requests_per_minute} req/min")
            
            # Calculer le temps d'attente n√©cessaire
            oldest_request = min(self.request_timestamps)
            wait_time = 60 - (now - oldest_request)
            
            if wait_time > 0:
                logger.info(f"‚è≥ Attente de {wait_time:.1f}s pour r√©initialiser le rate limit")
                time.sleep(wait_time)
                now = time.time()
                
                # Nettoyer √† nouveau apr√®s l'attente
                self.request_timestamps = [ts for ts in self.request_timestamps if now - ts < 60]
            
            return len(self.request_timestamps) < self.max_requests_per_minute
        
        # Enregistrer le timestamp et le temps de la derni√®re requ√™te
        self.request_timestamps.append(now)
        self.last_request_time = now
        return True
    
    def _handle_rate_limit_error(self, error_response: Dict) -> None:
        """Gestion intelligente des erreurs de rate limit avec backoff exponentiel"""
        if 'error' in error_response and 'type' in error_response['error']:
            error_type = error_response['error']['type']
            
            if 'rate_limit' in error_type.lower() or '429' in str(error_response):
                self.consecutive_failures += 1
                logger.warning(f"üö® Erreur rate limit d√©tect√©e (√©chec #{self.consecutive_failures})")
                
                # Strat√©gie de backoff exponentiel selon vos recommandations
                if self.consecutive_failures <= self.max_consecutive_failures:
                    backoff_time = min(self.max_backoff_time, self.backoff_multiplier ** self.consecutive_failures)
                    logger.info(f"‚è≥ Backoff exponentiel: attente de {backoff_time}s")
                    time.sleep(backoff_time)
                    
                    # R√©initialiser le rate limit
                    self.request_timestamps = []
                    self.rate_limit_reset_time = time.time() + 60
                else:
                    logger.error("üö® Trop d'√©checs cons√©cutifs, passage en mode fallback")
                    self.rate_limit_reset_time = time.time() + 300  # 5 minutes
            else:
                # R√©initialiser le compteur d'√©checs pour les autres erreurs
                self.consecutive_failures = 0
    
    def _can_retry_request(self) -> bool:
        """V√©rifie si on peut retenter une requ√™te apr√®s une erreur"""
        if self.consecutive_failures >= self.max_consecutive_failures:
            if self.rate_limit_reset_time and time.time() < self.rate_limit_reset_time:
                return False
            else:
                # R√©initialiser apr√®s la p√©riode de blocage
                self.consecutive_failures = 0
                self.rate_limit_reset_time = None
                return True
        return True
    
    def _call_mistral_api_with_retry(self, payload: Dict, max_retries: int = 3) -> Optional[Dict]:
        """Appel Mistral AI avec retry et gestion robuste des erreurs"""
        
        for attempt in range(max_retries):
            try:
                # V√©rification du rate limit
                if not self._check_rate_limit():
                    logger.warning(f"‚ö†Ô∏è Rate limit d√©pass√©, tentative {attempt + 1}/{max_retries}")
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)  # Backoff exponentiel
                        continue
                    else:
                        return None
                
                # Appel √† l'API Mistral
                headers = {
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "Accept": "application/json"
                }
                
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    ai_response = data['choices'][0]['message']['content']
                    
                    # Enregistrer le succ√®s
                    self.request_timestamps.append(time.time())
                    self.last_request_time = time.time()
                    self.consecutive_failures = 0  # R√©initialiser les √©checs
                    
                    logger.info(f"‚úÖ Appel Mistral AI r√©ussi (tentative {attempt + 1})")
                    return {
                        "response": ai_response,
                        "model": self.model,
                        "tokens_used": data['usage']['total_tokens'] if 'usage' in data else 0,
                        "attempt": attempt + 1
                    }
                    
                elif response.status_code == 429:  # Rate limit
                    logger.warning(f"üö® Rate limit Mistral AI (tentative {attempt + 1}/{max_retries})")
                    self._handle_rate_limit_error({"error": {"type": "rate_limit"}})
                    
                    if attempt < max_retries - 1:
                        backoff_time = min(60, 2 ** attempt)
                        logger.info(f"‚è≥ Attente de {backoff_time}s avant retry")
                        time.sleep(backoff_time)
                        continue
                    else:
                        return None
                        
                else:
                    logger.warning(f"‚ö†Ô∏è Erreur Mistral AI {response.status_code} (tentative {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        time.sleep(1)  # Attente courte pour les autres erreurs
                        continue
                    else:
                        return None
                        
            except requests.exceptions.Timeout:
                logger.warning(f"‚è∞ Timeout Mistral AI (tentative {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    return None
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur inattendue Mistral AI: {e} (tentative {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(1)
                    continue
                else:
                    return None
        
        return None
    
    def _get_language_context(self, language: str) -> str:
        """Retourne le contexte linguistique pour Mistral"""
        contexts = {
            'fr': "Tu es un assistant IA expert en transport public et boulangeries artisanales √† Paris. R√©ponds en fran√ßais de mani√®re professionnelle et bienveillante.",
            'en': "You are an AI assistant expert in public transport and artisanal bakeries in Paris. Respond in English in a professional and helpful manner.",
            'ja': "„ÅÇ„Å™„Åü„ÅØ„Éë„É™„ÅÆÂÖ¨ÂÖ±‰∫§ÈÄöÊ©üÈñ¢„Å®ËÅ∑‰∫∫„Éë„É≥Â±ã„ÅÆÂ∞ÇÈñÄÂÆ∂„Åß„ÅÇ„ÇãAI„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇÊó•Êú¨Ë™û„ÅßÂ∞ÇÈñÄÁöÑ„ÅßË¶™Âàá„Å´ÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
        }
        return contexts.get(language, contexts['fr'])
    
    def _get_transport_context(self, ratp_data: Dict) -> str:
        """G√©n√®re le contexte des transports pour Mistral"""
        if not ratp_data or ratp_data.get('source') == 'erreur_api_fallback':
            return "Informations de transport non disponibles actuellement."
        
        context = "Informations RATP temps r√©el :\n"
        
        # Statut des lignes
        lines_status = ratp_data.get('lines_status', [])
        if lines_status:
            context += "- Statut des lignes :\n"
            for line in lines_status[:5]:  # Limiter √† 5 lignes
                context += f"  ‚Ä¢ {line.get('line', 'N/A')} : {line.get('status', 'N/A')}\n"
        
        # Affluence des stations
        stations_crowding = ratp_data.get('stations_crowding', [])
        if stations_crowding:
            context += "- Affluence des stations :\n"
            for station in stations_crowding[:3]:  # Limiter √† 3 stations
                context += f"  ‚Ä¢ {station.get('station', 'N/A')} : {station.get('crowding', 'N/A')}\n"
        
        return context
    
    def _get_bakery_context(self, bakeries: List[Dict]) -> str:
        """G√©n√®re le contexte des boulangeries pour Mistral"""
        if not bakeries:
            return "Aucune boulangerie trouv√©e sur cet itin√©raire."
        
        context = "Boulangeries artisanales trouv√©es :\n"
        for bakery in bakeries[:3]:  # Limiter √† 3 boulangeries
            context += f"‚Ä¢ {bakery.get('name', 'N/A')} "
            context += f"(Note: {bakery.get('rating', 'N/A')}/5, "
            context += f"Distance: {bakery.get('distance', 'N/A')})\n"
        
        return context
    
    def generate_travel_advice(self, 
                              origin: str, 
                              destination: str, 
                              eta: str, 
                              distance: str,
                              bakeries: List[Dict],
                              ratp_data: Dict,
                              language: str = "fr") -> Optional[Dict]:
        """
        G√©n√®re des conseils de voyage personnalis√©s via Mistral AI
        
        Args:
            origin: Adresse de d√©part
            destination: Adresse d'arriv√©e
            eta: Temps estim√© du trajet
            distance: Distance du trajet
            bakeries: Liste des boulangeries trouv√©es
            ratp_data: Donn√©es RATP temps r√©el
            language: Langue de r√©ponse (fr, en, ja)
        
        Returns:
            Dict avec les conseils IA ou None si erreur
        """
        if not self.api_key:
            logger.warning("‚ö†Ô∏è Cl√© Mistral AI non configur√©e")
            return None
        
        if not self._check_rate_limit():
            logger.warning("‚ö†Ô∏è Rate limit d√©pass√©, utilisation du fallback")
            return None
        
        try:
            logger.info(f"ü§ñ G√©n√©ration conseils Mistral AI: {origin} ‚Üí {destination} (lang: {language})")
            
            # Construction du prompt contextuel
            system_prompt = self._get_language_context(language)
            system_prompt += "\n\nTu dois fournir des conseils utiles et personnalis√©s pour ce trajet."
            
            # Contexte du trajet
            route_context = f"""
Trajet : {origin} ‚Üí {destination}
Dur√©e estim√©e : {eta}
Distance : {distance}

{self._get_transport_context(ratp_data)}

{self._get_bakery_context(bakeries)}
"""
            
            # Prompt utilisateur
            user_prompt = f"""
Bas√© sur ces informations, donne-moi des conseils pratiques pour ce trajet :

1. Conseils de transport (horaires, alternatives, conseils d'affluence)
2. Recommandations pour les boulangeries (meilleur moment, sp√©cialit√©s)
3. Conseils g√©n√©raux pour optimiser ce trajet
4. Informations utiles sur l'itin√©raire

R√©ponds de mani√®re structur√©e et pratique.
"""
            
            # Pr√©paration de la requ√™te Mistral
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": route_context + user_prompt}
                ],
                "max_tokens": 1000,
                "temperature": 0.7,
                "top_p": 0.9,
                "stream": False
            }
            
            # Utilisation de la nouvelle m√©thode de retry
            result = self._call_mistral_api_with_retry(payload)
            
            if result:
                ai_response = result["response"]
                model = result["model"]
                tokens_used = result["tokens_used"]
                
                logger.info("‚úÖ Conseils Mistral AI g√©n√©r√©s avec succ√®s")
                
                return {
                    "ai_advice": ai_response,
                    "model": model,
                    "language": language,
                    "timestamp": time.time(),
                    "source": "mistral_ai_api",
                    "tokens_used": tokens_used,
                    "attempt": result["attempt"]
                }
                
            else:
                logger.warning("‚ö†Ô∏è Erreur lors de l'appel Mistral AI avec retry")
                return None
                
        except requests.exceptions.Timeout:
            logger.error("‚ùå Timeout Mistral AI API")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Erreur requ√™te Mistral AI: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue Mistral AI: {e}")
            return None
    
    def get_fallback_advice(self, 
                           origin: str, 
                           destination: str, 
                           eta: str, 
                           distance: str,
                           language: str = "fr") -> Dict:
        """Conseils de fallback intelligents quand Mistral n'est pas disponible"""
        logger.info(f"üîÑ Utilisation du fallback pour: {origin} ‚Üí {destination}")
        
        if language == 'en':
            advice = f"""
Travel Advice for {origin} ‚Üí {destination}:

üöá Transport Tips:
‚Ä¢ Estimated time: {eta}
‚Ä¢ Distance: {distance}
‚Ä¢ Consider taking the metro during off-peak hours
‚Ä¢ Check RATP app for real-time updates

ü•ñ Bakery Recommendations:
‚Ä¢ Visit bakeries early morning for fresh bread
‚Ä¢ Look for "artisan" bakeries for authentic French bread
‚Ä¢ Try traditional baguettes and croissants

üí° General Tips:
‚Ä¢ Plan your route in advance
‚Ä¢ Have a backup transport option
‚Ä¢ Enjoy the journey and discover new bakeries!
"""
        elif language == 'ja':
            advice = f"""
{origin} ‚Üí {destination} „Å∏„ÅÆÊóÖË°å„Ç¢„Éâ„Éê„Ç§„Çπ:

üöá ‰∫§ÈÄö„ÅÆ„Éí„É≥„Éà:
‚Ä¢ ‰∫àÊÉ≥ÊôÇÈñì: {eta}
‚Ä¢ Ë∑ùÈõ¢: {distance}
‚Ä¢ „É©„ÉÉ„Ç∑„É•„Ç¢„ÉØ„Éº„ÇíÈÅø„Åë„Å¶„É°„Éà„É≠„ÇíÂà©Áî®
‚Ä¢ RATP„Ç¢„Éó„É™„Åß„É™„Ç¢„É´„Çø„Ç§„É†ÊÉÖÂ†±„ÇíÁ¢∫Ë™ç

ü•ñ „Éô„Éº„Ç´„É™„Éº„ÅÆÊé®Â•®:
‚Ä¢ ÊúùÊó©„Åè„Å´Ë®™„Çå„Å¶Êñ∞ÈÆÆ„Å™„Éë„É≥„ÇíË≥ºÂÖ•
‚Ä¢ Êú¨Ê†ºÁöÑ„Å™„Éï„É©„É≥„Çπ„Éë„É≥„ÅÆ„Åü„ÇÅ„Å´„ÄåËÅ∑‰∫∫„Äç„Éô„Éº„Ç´„É™„Éº„ÇíÊé¢„Åô
‚Ä¢ ‰ºùÁµ±ÁöÑ„Å™„Éê„Ç≤„ÉÉ„Éà„Å®„ÇØ„É≠„ÉØ„ÉÉ„Çµ„É≥„ÇíË©¶„Åô

üí° ‰∏ÄËà¨ÁöÑ„Å™„Éí„É≥„Éà:
‚Ä¢ ‰∫ãÂâç„Å´„É´„Éº„Éà„ÇíË®àÁîª
‚Ä¢ „Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„ÅÆ‰∫§ÈÄöÊâãÊÆµ„ÇíÁî®ÊÑè
‚Ä¢ ÊóÖ„ÇíÊ•Ω„Åó„Åø„ÄÅÊñ∞„Åó„ÅÑ„Éô„Éº„Ç´„É™„Éº„ÇíÁô∫Ë¶ãÔºÅ
"""
        else:  # Fran√ßais par d√©faut
            advice = f"""
Conseils de voyage pour {origin} ‚Üí {destination}:

üöá Conseils de transport:
‚Ä¢ Temps estim√© : {eta}
‚Ä¢ Distance : {distance}
‚Ä¢ Privil√©giez le m√©tro en heures creuses
‚Ä¢ Consultez l'app RATP pour les mises √† jour temps r√©el

ü•ñ Recommandations boulangeries:
‚Ä¢ Visitez les boulangeries t√¥t le matin pour du pain frais
‚Ä¢ Recherchez les boulangeries "artisanales" pour du vrai pain fran√ßais
‚Ä¢ Testez les baguettes et croissants traditionnels

üí° Conseils g√©n√©raux:
‚Ä¢ Planifiez votre itin√©raire √† l'avance
‚Ä¢ Ayez une option de transport de secours
‚Ä¢ Profitez du trajet et d√©couvrez de nouvelles boulangeries !
"""
        
        return {
            "ai_advice": advice,
            "model": "fallback_intelligent",
            "language": language,
            "timestamp": time.time(),
            "source": "fallback_intelligent"
        }
    
    def generate_simple_chat_response(self, message: str, language: str = "fr") -> Optional[str]:
        """G√©n√®re une r√©ponse simple de chat via Mistral AI avec gestion robuste du rate limit"""
        try:
            # V√©rification de la configuration
            if not self.api_key:
                logger.warning("‚ö†Ô∏è Cl√© Mistral AI non configur√©e")
                return None

            # Construction du prompt contextuel pour le chat simple
            system_prompt = self._get_language_context(language)
            system_prompt += "\n\nTu es un assistant conciergerie expert en transport public et boulangeries artisanales √† Paris. "
            system_prompt += "R√©ponds de mani√®re utile, bienveillante et pratique. "
            system_prompt += "Donne des r√©ponses d√©taill√©es et contextuelles, pas juste des phrases courtes."

            # Prompt utilisateur enrichi
            user_prompt = f"""Question de l'utilisateur: {message}

Instructions:
1. R√©ponds dans la langue de la question
2. Donne des conseils pratiques et d√©taill√©s
3. Mentionne les options de transport (RER B, M√©tro, Bus)
4. Sugg√®re des boulangeries artisanales si pertinent
5. Sois utile et informatif, pas g√©n√©rique

R√©ponds de mani√®re pratique et informative avec des d√©tails concrets."""

            # Pr√©paration de la requ√™te Mistral
            payload = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "max_tokens": 500,
                "temperature": 0.7,
                "top_p": 0.9,
                "stream": False
            }

            # Utilisation de la nouvelle m√©thode de retry robuste
            result = self._call_mistral_api_with_retry(payload)
            
            if result:
                ai_response = result["response"]
                logger.info(f"‚úÖ R√©ponse Mistral AI g√©n√©r√©e avec succ√®s (tentative {result['attempt']})")
                return ai_response
            else:
                logger.warning("‚ö†Ô∏è √âchec de l'appel Mistral AI apr√®s tous les retry")
                return None

        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue dans generate_simple_chat_response: {e}")
            return None

# Instance globale du service
mistral_ai_service = MistralAIService()
