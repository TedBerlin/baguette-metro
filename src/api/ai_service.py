#!/usr/bin/env python3
"""
Service IA Mistral direct + OpenRouter fallback + Local fallback
"""

import aiohttp
import asyncio
from config import (
    MISTRAL_API_KEY, OPENROUTER_API_KEY, OPENAI_API_KEY,
    MISTRAL_MODEL, OPENROUTER_MODEL, OPENAI_MODEL,
    MISTRAL_API_URL, OPENROUTER_API_URL, OPENAI_API_URL,
    API_TIMEOUT, MAX_TOKENS, TEMPERATURE, TOP_P,
    ENABLE_MISTRAL_DIRECT, ENABLE_OPENAI_FALLBACK, ENABLE_OPENROUTER_FALLBACK, ENABLE_LOCAL_FALLBACK
)

async def call_mistral_direct(message: str, language: str) -> str:
    """Appelle directement l'API Mistral (prioritÃ© 1)"""
    if not ENABLE_MISTRAL_DIRECT:
        return ""
        
    try:
        if not MISTRAL_API_KEY:
            raise Exception("ClÃ© API Mistral non configurÃ©e")
        
        # Construction du prompt contextuel avec instruction de langue STRICTE
        if language == "fr":
            system_prompt = f"""Tu es un assistant concierge franÃ§ais pour 'Baguette & MÃ©tro'.
RÃˆGLE ABSOLUE: Tu DOIS rÃ©pondre UNIQUEMENT en franÃ§ais.
Si tu rÃ©ponds en anglais, tu seras pÃ©nalisÃ©.
Tu es un assistant franÃ§ais, tu parles franÃ§ais, tu penses en franÃ§ais.
RÃ‰PONDS EN FRANÃ‡AIS SEULEMENT."""
        else:
            system_prompt = f"Tu es un assistant concierge pour 'Baguette & MÃ©tro'. RÃˆGLE ABSOLUE: Tu DOIS rÃ©pondre UNIQUEMENT en {language}. Si la langue est 'en', rÃ©ponds en anglais. Si la langue est 'ja', rÃ©ponds en japonais."
        
        user_prompt = f"IMPORTANT: RÃ©ponds UNIQUEMENT en {language}. Question: {message}"
        
        # Appel API Mistral direct avec system message
        async with aiohttp.ClientSession() as session:
            async with session.post(
                MISTRAL_API_URL,
                headers={
                    "Authorization": f"Bearer {MISTRAL_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": MISTRAL_MODEL,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE,
                    "top_p": TOP_P
                },
                timeout=aiohttp.ClientTimeout(total=API_TIMEOUT)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Erreur API Mistral: {response.status}")
                
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()
                
    except Exception as e:
        print(f"Erreur Mistral direct: {str(e)}")
        return ""

async def call_openai_api(message: str, language: str) -> str:
    """Appelle l'API OpenAI (prioritÃ© 2)"""
    if not ENABLE_OPENAI_FALLBACK:
        return ""
        
    try:
        if not OPENAI_API_KEY:
            raise Exception("ClÃ© API OpenAI non configurÃ©e")
        
        # Construction du prompt contextuel avec instruction de langue STRICTE
        system_prompt = f"Tu es un assistant concierge pour 'Baguette & MÃ©tro'. RÃˆGLE ABSOLUE: Tu DOIS rÃ©pondre UNIQUEMENT en {language}. Si la langue est 'fr', rÃ©ponds en franÃ§ais. Si la langue est 'en', rÃ©ponds en anglais. Si la langue est 'ja', rÃ©ponds en japonais."
        
        user_prompt = f"IMPORTANT: RÃ©ponds UNIQUEMENT en {language}. Question: {message}"
        
        # Appel API OpenAI avec system message
        async with aiohttp.ClientSession() as session:
            async with session.post(
                OPENAI_API_URL,
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": OPENAI_MODEL,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE,
                    "top_p": TOP_P
                },
                timeout=aiohttp.ClientTimeout(total=API_TIMEOUT)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Erreur API OpenAI: {response.status}")
                
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()
                
    except Exception as e:
        print(f"Erreur OpenAI: {str(e)}")
        return ""

async def call_openrouter_api(message: str, language: str) -> str:
    """Appelle l'API OpenRouter (prioritÃ© 3)"""
    if not ENABLE_OPENROUTER_FALLBACK:
        return ""
        
    try:
        if not OPENROUTER_API_KEY:
            raise Exception("ClÃ© API OpenRouter non configurÃ©e")
        
        # Construction du prompt contextuel
        prompt = build_openrouter_prompt(message, language)
        
        # Appel API avec timeout
        async with aiohttp.ClientSession() as session:
            async with session.post(
                OPENROUTER_API_URL,
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": OPENROUTER_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE,
                    "top_p": TOP_P
                },
                timeout=aiohttp.ClientTimeout(total=API_TIMEOUT)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Erreur API OpenRouter: {response.status}")
                
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()
                
    except Exception as e:
        print(f"Erreur OpenRouter: {str(e)}")
        return ""

async def call_ai_with_fallback(message: str, language: str) -> tuple[str, str]:
    """Appelle l'IA avec fallback intelligent : Mistral â†’ OpenRouter â†’ Local"""
    try:
        # STRATÃ‰GIE HYBRIDE INTELLIGENTE PAR LANGUE
        if language == "fr":
            # FRANÃ‡AIS : OpenAI direct (rÃ©sout le problÃ¨me de langue)
            print(f"ğŸ”µ FRANÃ‡AIS DÃ‰TECTÃ‰ - Utilisation d'OpenAI direct")
            print(f"ğŸ” DEBUG: ENABLE_OPENAI_FALLBACK = {ENABLE_OPENAI_FALLBACK}")
            print(f"ğŸ” DEBUG: OPENAI_API_KEY = {'CONFIGURÃ‰' if OPENAI_API_KEY else 'NON CONFIGURÃ‰'}")
            
            if ENABLE_OPENAI_FALLBACK:
                print(f"ğŸš€ TENTATIVE OPENAI...")
                openai_response = await call_openai_api(message, language)
                print(f"ğŸ” DEBUG: RÃ©ponse OpenAI = '{openai_response}'")
                if openai_response and openai_response.strip():
                    print(f"âœ… OPENAI RÃ‰USSI !")
                    return openai_response, "openai_direct"
                else:
                    print(f"âŒ OPENAI Ã‰CHOUÃ‰ - Fallback local")
            # Fallback local si OpenAI Ã©choue
            if ENABLE_LOCAL_FALLBACK:
                local_response = get_intelligent_fallback(message, language)
                return local_response, "local_fallback_fr"
        elif language == "ja":
            # JAPONAIS : Mistral direct (fonctionne parfaitement)
            print(f"ğŸŸ¡ JAPONAIS DÃ‰TECTÃ‰ - Utilisation de Mistral direct")
            if ENABLE_MISTRAL_DIRECT:
                mistral_response = await call_mistral_direct(message, language)
                if mistral_response and mistral_response.strip():
                    return mistral_response, "mistral_direct"
        elif language == "en":
            # ANGLAIS : Mistral direct (fonctionne parfaitement)
            print(f"ğŸ”´ ANGLAIS DÃ‰TECTÃ‰ - Utilisation de Mistral direct")
            if ENABLE_MISTRAL_DIRECT:
                mistral_response = await call_mistral_direct(message, language)
                if mistral_response and mistral_response.strip():
                    return mistral_response, "mistral_direct"
        
        # 2. TENTATIVE OPENAI (fallback gÃ©nÃ©ral pour toutes les langues)
        if ENABLE_OPENAI_FALLBACK:
            openai_response = await call_openai_api(message, language)
            if openai_response and openai_response.strip():
                return openai_response, "openai_fallback"
        
        # 3. TENTATIVE OPENROUTER (fallback gÃ©nÃ©ral)
        if ENABLE_OPENROUTER_FALLBACK:
            openrouter_response = await call_openrouter_api(message, language)
            if openrouter_response and openrouter_response.strip():
                return openrouter_response, "openrouter"
        
        # 4. FALLBACK LOCAL (sÃ©curitÃ©)
        if ENABLE_LOCAL_FALLBACK:
            local_response = get_intelligent_fallback(message, language)
            return local_response, "local_fallback"
        
        # Si aucun fallback n'est activÃ©
        return "Service temporairement indisponible", "error"
        
    except Exception as e:
        print(f"Erreur dans l'appel IA: {str(e)}")
        if ENABLE_LOCAL_FALLBACK:
            local_response = get_intelligent_fallback(message, language)
            return local_response, "local_fallback"
        return "Erreur systÃ¨me", "error"

def build_mistral_prompt(message: str, language: str) -> str:
    """Construit un prompt intelligent pour Mistral direct"""
    base_context = {
        "fr": "Tu es un assistant concierge expert pour 'Baguette & MÃ©tro', une application qui optimise les trajets RATP avec arrÃªts boulangerie. Sois concis, utile et Ã©vite de rÃ©pÃ©ter ce que dit l'utilisateur. IMPORTANT: RÃ©ponds UNIQUEMENT en franÃ§ais.",
        "en": "You are a concierge assistant for 'Baguette & MÃ©tro', an app that optimizes RATP routes with bakery stops. Be concise, helpful and avoid repeating the user's words. IMPORTANT: Respond ONLY in English.",
        "ja": "ã‚ãªãŸã¯ã€Œãƒã‚²ãƒƒãƒˆãƒ»ãƒ¡ãƒˆãƒ­ã€ã®ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚RATPã®è·¯ç·šã‚’ãƒ‘ãƒ³å±‹å¯„ã‚Šé“ã§æœ€é©åŒ–ã™ã‚‹ã‚¢ãƒ—ãƒªã§ã™ã€‚ç°¡æ½”ã§å½¹ç«‹ã¤å¿œç­”ã‚’ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨€è‘‰ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚é‡è¦ï¼šæ—¥æœ¬èªã§ã®ã¿å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    }
    
    context = base_context.get(language, base_context["fr"])
    
    return f"""{context}

Message de l'utilisateur: "{message}"
Instructions importantes:
- NE RÃ‰PÃˆTE PAS le message de l'utilisateur
- Sois concis et utile
- RÃ‰PONDS UNIQUEMENT DANS LA LANGUE: {language}
- Context: assistant concierge Baguette & MÃ©tro

RÃ‰PONSE EN {language.upper()}:"""

def build_openrouter_prompt(message: str, language: str) -> str:
    """Construit un prompt intelligent pour OpenRouter"""
    base_context = {
        "fr": "Tu es un assistant concierge expert pour 'Baguette & MÃ©tro', une application qui optimise les trajets RATP avec arrÃªts boulangerie. Sois concis, utile et Ã©vite de rÃ©pÃ©ter ce que dit l'utilisateur. IMPORTANT: RÃ©ponds UNIQUEMENT en franÃ§ais.",
        "en": "You are a concierge assistant for 'Baguette & MÃ©tro', an app that optimizes RATP routes with bakery stops. Be concise, helpful and avoid repeating the user's words. IMPORTANT: Respond ONLY in English.",
        "ja": "ã‚ãªãŸã¯ã€Œãƒã‚²ãƒƒãƒˆãƒ»ãƒ¡ãƒˆãƒ­ã€ã®ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚RATPã®è·¯ç·šã‚’ãƒ‘ãƒ³å±‹å¯„ã‚Šé“ã§æœ€é©åŒ–ã™ã‚‹ã‚¢ãƒ—ãƒªã§ã™ã€‚ç°¡æ½”ã§å½¹ç«‹ã¤å¿œç­”ã‚’ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¨€è‘‰ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„ã€‚é‡è¦ï¼šæ—¥æœ¬èªã§ã®ã¿å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    }
    
    context = base_context.get(language, base_context["fr"])
    
    return f"""{context}

Message de l'utilisateur: "{message}"
Instructions importantes:
- NE RÃ‰PÃˆTE PAS le message de l'utilisateur
- Sois concis et utile
- RÃ‰PONDS UNIQUEMENT DANS LA LANGUE: {language}
- Context: assistant concierge Baguette & MÃ©tro

RÃ‰PONSE EN {language.upper()}:"""

def get_fallback_response(context: str, language: str) -> str:
    """Fallback simple pour messages vides"""
    fallbacks = {
        "fr": "Comment puis-je vous aider aujourd'hui ?",
        "en": "How can I help you today?",
        "ja": "ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã”ç”¨ä»¶ã§ã™ã‹ï¼Ÿ"
    }
    return fallbacks.get(language, fallbacks["fr"])

def get_intelligent_fallback(message: str, language: str) -> str:
    """Fallback intelligent basÃ© sur le contexte du message"""
    message_lower = message.lower()
    
    # DÃ©tection du contexte et rÃ©ponse intelligente
    if any(word in message_lower for word in ['comment', 'how', 'ã©ã†', 'fonctionne', 'work']):
        return get_context_response("how_it_works", language)
    elif any(word in message_lower for word in ['boulangerie', 'bakery', 'ãƒ‘ãƒ³å±‹', 'pain', 'croissant']):
        return get_context_response("bakery", language)
    elif any(word in message_lower for word in ['mÃ©tro', 'metro', 'ãƒ¡ãƒˆãƒ­', 'transport', 'bus', 'ratp']):
        return get_context_response("metro", language)
    elif any(word in message_lower for word in ['paris', 'visiter', 'tourisme', 'voyage', 'visit', 'tourism', 'first time']):
        return get_context_response("paris_tourism", language)
    elif any(word in message_lower for word in ['bonjour', 'salut', 'hello', 'hi', 'ã“ã‚“ã«ã¡ã¯']):
        return get_context_response("greeting", language)
    else:
        return get_context_response("default", language)

def get_context_response(context: str, language: str) -> str:
    """Retourne la rÃ©ponse contextuelle dans la langue appropriÃ©e"""
    responses = {
        "fr": {
            "how_it_works": "ğŸ¥– Baguette & MÃ©tro fonctionne ainsi : 1) Entrez votre dÃ©part et destination, 2) Cliquez sur 'Calculer l'itinÃ©raire', 3) L'IA trouve le meilleur trajet avec des boulangeries sur le chemin ! ğŸš‡",
            "bakery": "ğŸ¥– Les boulangeries parisiennes sont excellentes ! Notre app trouve automatiquement les meilleures sur votre trajet. Essayez de calculer un itinÃ©raire pour voir ! ğŸ—ºï¸",
            "metro": "ğŸš‡ Le mÃ©tro parisien est trÃ¨s pratique ! Notre app utilise l'API PRIM RATP pour des donnÃ©es en temps rÃ©el. Calculons ensemble votre trajet optimal ! ğŸš€",
            "paris_tourism": "ğŸ›ï¸ Paris est magnifique ! Notre app vous aide Ã  planifier vos dÃ©placements et dÃ©couvrir les meilleures boulangeries. Voulez-vous calculer un itinÃ©raire ? ğŸ¥–",
            "greeting": "Bonjour ! ğŸ‘‹ Je suis votre assistant Baguette & MÃ©tro. Je peux vous aider avec les transports parisiens et les boulangeries. Que souhaitez-vous savoir ? ğŸš‡ğŸ¥–",
            "default": "ğŸ¤– Je peux vous aider avec les transports parisiens, les boulangeries et la planification d'itinÃ©raires. Posez-moi une question spÃ©cifique ! ğŸš‡ğŸ¥–"
        },
        "en": {
            "how_it_works": "ğŸ¥– Baguette & MÃ©tro works like this: 1) Enter your departure and destination, 2) Click 'Calculate Route', 3) AI finds the best route with bakeries along the way! ğŸš‡",
            "bakery": "ğŸ¥– Parisian bakeries are excellent! Our app automatically finds the best ones on your route. Try calculating a route to see! ğŸ—ºï¸",
            "metro": "ğŸš‡ The Paris metro is very convenient! Our app uses the PRIM RATP API for real-time data. Let's calculate your optimal route together! ğŸš€",
            "paris_tourism": "ğŸ›ï¸ Paris is beautiful! Our app helps you plan your trips and discover the best bakeries. Would you like to calculate a route? ğŸ¥–",
            "greeting": "Hello! ğŸ‘‹ I'm your Baguette & MÃ©tro assistant. I can help you with Parisian transport and bakeries. What would you like to know? ğŸš‡ğŸ¥–",
            "default": "ğŸ¤– I can help you with Parisian transport, bakeries and route planning. Ask me a specific question! ğŸš‡ğŸ¥–"
        },
        "ja": {
            "how_it_works": "ğŸ¥– ãƒã‚²ãƒƒãƒˆãƒ»ãƒ¡ãƒˆãƒ­ã¯ã“ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ï¼š1) å‡ºç™ºåœ°ã¨ç›®çš„åœ°ã‚’å…¥åŠ›ã€2) ã€Œãƒ«ãƒ¼ãƒˆè¨ˆç®—ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã€3) AIãŒãƒ‘ãƒ³å±‹ã•ã‚“å¯„ã‚Šé“ã®æœ€é©ãƒ«ãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã¾ã™ï¼ğŸš‡",
            "bakery": "ğŸ¥– ãƒ‘ãƒªã®ãƒ‘ãƒ³å±‹ã•ã‚“ã¯ç´ æ™´ã‚‰ã—ã„ã§ã™ï¼ç§ãŸã¡ã®ã‚¢ãƒ—ãƒªãŒè‡ªå‹•çš„ã«ãƒ«ãƒ¼ãƒˆä¸Šã®æœ€é«˜ã®ãƒ‘ãƒ³å±‹ã•ã‚“ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚ãƒ«ãƒ¼ãƒˆã‚’è¨ˆç®—ã—ã¦ã¿ã¦ãã ã•ã„ï¼ğŸ—ºï¸",
            "metro": "ğŸš‡ ãƒ‘ãƒªã®ãƒ¡ãƒˆãƒ­ã¯ã¨ã¦ã‚‚ä¾¿åˆ©ã§ã™ï¼ç§ãŸã¡ã®ã‚¢ãƒ—ãƒªã¯PRIM RATP APIã‚’ä½¿ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚ä¸€ç·’ã«æœ€é©ãªãƒ«ãƒ¼ãƒˆã‚’è¨ˆç®—ã—ã¾ã—ã‚‡ã†ï¼ğŸš€",
            "paris_tourism": "ğŸ›ï¸ ãƒ‘ãƒªã¯ç¾ã—ã„è¡—ã§ã™ï¼ç§ãŸã¡ã®ã‚¢ãƒ—ãƒªãŒæ—…è¡Œè¨ˆç”»ã¨æœ€é«˜ã®ãƒ‘ãƒ³å±‹ã•ã‚“ç™ºè¦‹ã‚’ãŠæ‰‹ä¼ã„ã—ã¾ã™ã€‚ãƒ«ãƒ¼ãƒˆã‚’è¨ˆç®—ã—ã¾ã™ã‹ï¼ŸğŸ¥–",
            "greeting": "ã“ã‚“ã«ã¡ã¯ï¼ğŸ‘‹ ãƒã‚²ãƒƒãƒˆãƒ»ãƒ¡ãƒˆãƒ­ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ‘ãƒªã®äº¤é€šæ©Ÿé–¢ã¨ãƒ‘ãƒ³å±‹ã•ã‚“ã«ã¤ã„ã¦ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã€‚ä½•ã‚’çŸ¥ã‚ŠãŸã„ã§ã™ã‹ï¼ŸğŸš‡ğŸ¥–",
            "default": "ğŸ¤– ãƒ‘ãƒªã®äº¤é€šæ©Ÿé–¢ã€ãƒ‘ãƒ³å±‹ã•ã‚“ã€ãƒ«ãƒ¼ãƒˆè¨ˆç”»ã«ã¤ã„ã¦ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã€‚å…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ãã ã•ã„ï¼ğŸš‡ğŸ¥–"
        }
    }
    
    return responses.get(language, responses["fr"]).get(context, responses["fr"]["default"])

def detect_language_from_message(message: str) -> str:
    """DÃ©tecte automatiquement la langue de la question"""
    message_lower = message.lower()
    
    # DÃ©tection anglaise
    english_words = ['what', 'how', 'where', 'when', 'why', 'first', 'time', 'paris', 'visit', 'tour', 'help', 'hello', 'hi']
    if any(word in message_lower for word in english_words):
        return "en"
    
    # DÃ©tection japonaise
    japanese_chars = ['ã¯', 'ãŒ', 'ã‚’', 'ã«', 'ã¸', 'ã§', 'ã®', 'ã‹', 'ã‚ˆ', 'ã­', 'ã‚', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‚']
    if any(char in message for char in japanese_chars):
        return "ja"
    
    # Par dÃ©faut : franÃ§ais
    return "fr"
