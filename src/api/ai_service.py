#!/usr/bin/env python3
"""
Service IA Mistral direct + OpenRouter fallback + Local fallback
"""

import aiohttp
import asyncio
from config import (
    MISTRAL_API_KEY, OPENROUTER_API_KEY, OPENAI_API_KEY,
    MISTRAL_MODEL, OPENROUTER_MODEL, OPENAI_MODEL,
    MISTRAL_API_URL, OPENROUTER_API_URL, OPENAI_API_URL,
    API_TIMEOUT, MAX_TOKENS, TEMPERATURE, TOP_P,
    ENABLE_MISTRAL_DIRECT, ENABLE_OPENAI_FALLBACK, ENABLE_OPENROUTER_FALLBACK, ENABLE_LOCAL_FALLBACK
)

async def call_mistral_direct(message: str, language: str) -> str:
    """Appelle directement l'API Mistral (priorit√© 1)"""
    if not ENABLE_MISTRAL_DIRECT:
        return ""
        
    try:
        if not MISTRAL_API_KEY:
            raise Exception("Cl√© API Mistral non configur√©e")
        
        # Construction du prompt contextuel avec instruction de langue STRICTE
        if language == "fr":
            system_prompt = f"""Tu es un assistant concierge fran√ßais pour 'Baguette & M√©tro'.
R√àGLE ABSOLUE: Tu DOIS r√©pondre UNIQUEMENT en fran√ßais.
Si tu r√©ponds en anglais, tu seras p√©nalis√©.
Tu es un assistant fran√ßais, tu parles fran√ßais, tu penses en fran√ßais.
R√âPONDS EN FRAN√áAIS SEULEMENT."""
        else:
            system_prompt = f"Tu es un assistant concierge pour 'Baguette & M√©tro'. R√àGLE ABSOLUE: Tu DOIS r√©pondre UNIQUEMENT en {language}. Si la langue est 'en', r√©ponds en anglais. Si la langue est 'ja', r√©ponds en japonais."
        
        user_prompt = f"IMPORTANT: R√©ponds UNIQUEMENT en {language}. Question: {message}"
        
        # Appel API Mistral direct avec system message
        async with aiohttp.ClientSession() as session:
            async with session.post(
                MISTRAL_API_URL,
                headers={
                    "Authorization": f"Bearer {MISTRAL_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": MISTRAL_MODEL,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE,
                    "top_p": TOP_P
                },
                timeout=aiohttp.ClientTimeout(total=API_TIMEOUT)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Erreur API Mistral: {response.status}")
                
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()
                
    except Exception as e:
        print(f"Erreur Mistral direct: {str(e)}")
        return ""

async def call_openai_api(message: str, language: str) -> str:
    """Appelle l'API OpenAI (priorit√© 2)"""
    if not ENABLE_OPENAI_FALLBACK:
        return ""
        
    try:
        if not OPENAI_API_KEY:
            raise Exception("Cl√© API OpenAI non configur√©e")
        
        # Construction du prompt contextuel avec instruction de langue STRICTE
        system_prompt = f"Tu es un assistant concierge pour 'Baguette & M√©tro'. R√àGLE ABSOLUE: Tu DOIS r√©pondre UNIQUEMENT en {language}. Si la langue est 'fr', r√©ponds en fran√ßais. Si la langue est 'en', r√©ponds en anglais. Si la langue est 'ja', r√©ponds en japonais."
        
        user_prompt = f"IMPORTANT: R√©ponds UNIQUEMENT en {language}. Question: {message}"
        
        # Appel API OpenAI avec system message
        async with aiohttp.ClientSession() as session:
            async with session.post(
                OPENAI_API_URL,
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": OPENAI_MODEL,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE,
                    "top_p": TOP_P
                },
                timeout=aiohttp.ClientTimeout(total=API_TIMEOUT)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Erreur API OpenAI: {response.status}")
                
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()
                
    except Exception as e:
        print(f"Erreur OpenAI: {str(e)}")
        return ""

async def call_openrouter_api(message: str, language: str) -> str:
    """Appelle l'API OpenRouter (priorit√© 3)"""
    if not ENABLE_OPENROUTER_FALLBACK:
        return ""
        
    try:
        if not OPENROUTER_API_KEY:
            raise Exception("Cl√© API OpenRouter non configur√©e")
        
        # Construction du prompt contextuel
        prompt = build_openrouter_prompt(message, language)
        
        # Appel API avec timeout
        async with aiohttp.ClientSession() as session:
            async with session.post(
                OPENROUTER_API_URL,
                headers={
                    "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": OPENROUTER_MODEL,
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": MAX_TOKENS,
                    "temperature": TEMPERATURE,
                    "top_p": TOP_P
                },
                timeout=aiohttp.ClientTimeout(total=API_TIMEOUT)
            ) as response:
                if response.status != 200:
                    raise Exception(f"Erreur API OpenRouter: {response.status}")
                
                data = await response.json()
                return data["choices"][0]["message"]["content"].strip()
                
    except Exception as e:
        print(f"Erreur OpenRouter: {str(e)}")
        return ""

async def call_ai_with_fallback(message: str, language: str) -> tuple[str, str]:
    """Appelle l'IA avec fallback intelligent : Mistral ‚Üí OpenRouter ‚Üí Local"""
    try:
        # STRAT√âGIE HYBRIDE INTELLIGENTE PAR LANGUE
        if language == "fr":
            # FRAN√áAIS : OpenAI direct (r√©sout le probl√®me de langue)
            print(f"üîµ FRAN√áAIS D√âTECT√â - Utilisation d'OpenAI direct")
            print(f"üîç DEBUG: ENABLE_OPENAI_FALLBACK = {ENABLE_OPENAI_FALLBACK}")
            print(f"üîç DEBUG: OPENAI_API_KEY = {'CONFIGUR√â' if OPENAI_API_KEY else 'NON CONFIGUR√â'}")
            
            if ENABLE_OPENAI_FALLBACK:
                print(f"üöÄ TENTATIVE OPENAI...")
                openai_response = await call_openai_api(message, language)
                print(f"üîç DEBUG: R√©ponse OpenAI = '{openai_response}'")
                if openai_response and openai_response.strip():
                    print(f"‚úÖ OPENAI R√âUSSI !")
                    return openai_response, "openai_direct"
                else:
                    print(f"‚ùå OPENAI √âCHOU√â - Fallback local")
            # Fallback local si OpenAI √©choue
            if ENABLE_LOCAL_FALLBACK:
                local_response = get_intelligent_fallback(message, language)
                return local_response, "local_fallback_fr"
        elif language == "ja":
            # JAPONAIS : Mistral direct (fonctionne parfaitement)
            print(f"üü° JAPONAIS D√âTECT√â - Utilisation de Mistral direct")
            if ENABLE_MISTRAL_DIRECT:
                mistral_response = await call_mistral_direct(message, language)
                if mistral_response and mistral_response.strip():
                    return mistral_response, "mistral_direct"
        elif language == "en":
            # ANGLAIS : Mistral direct (fonctionne parfaitement)
            print(f"üî¥ ANGLAIS D√âTECT√â - Utilisation de Mistral direct")
            if ENABLE_MISTRAL_DIRECT:
                mistral_response = await call_mistral_direct(message, language)
                if mistral_response and mistral_response.strip():
                    return mistral_response, "mistral_direct"
        
        # 2. TENTATIVE OPENAI (fallback g√©n√©ral pour toutes les langues)
        if ENABLE_OPENAI_FALLBACK:
            openai_response = await call_openai_api(message, language)
            if openai_response and openai_response.strip():
                return openai_response, "openai_fallback"
        
        # 3. TENTATIVE OPENROUTER (fallback g√©n√©ral)
        if ENABLE_OPENROUTER_FALLBACK:
            openrouter_response = await call_openrouter_api(message, language)
            if openrouter_response and openrouter_response.strip():
                return openrouter_response, "openrouter"
        
        # 4. FALLBACK LOCAL (s√©curit√©)
        if ENABLE_LOCAL_FALLBACK:
            local_response = get_intelligent_fallback(message, language)
            return local_response, "local_fallback"
        
        # Si aucun fallback n'est activ√©
        return "Service temporairement indisponible", "error"
        
    except Exception as e:
        print(f"Erreur dans l'appel IA: {str(e)}")
        if ENABLE_LOCAL_FALLBACK:
            local_response = get_intelligent_fallback(message, language)
            return local_response, "local_fallback"
        return "Erreur syst√®me", "error"

def build_mistral_prompt(message: str, language: str) -> str:
    """Construit un prompt intelligent pour Mistral direct"""
    base_context = {
        "fr": "Tu es un assistant concierge expert pour 'Baguette & M√©tro', une application qui optimise les trajets RATP avec arr√™ts boulangerie. Sois concis, utile et √©vite de r√©p√©ter ce que dit l'utilisateur. IMPORTANT: R√©ponds UNIQUEMENT en fran√ßais.",
        "en": "You are a concierge assistant for 'Baguette & M√©tro', an app that optimizes RATP routes with bakery stops. Be concise, helpful and avoid repeating the user's words. IMPORTANT: Respond ONLY in English.",
        "ja": "„ÅÇ„Å™„Åü„ÅØ„Äå„Éê„Ç≤„ÉÉ„Éà„Éª„É°„Éà„É≠„Äç„ÅÆ„Ç≥„É≥„Ç∑„Çß„É´„Ç∏„É•„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇRATP„ÅÆË∑ØÁ∑ö„Çí„Éë„É≥Â±ãÂØÑ„ÇäÈÅì„ÅßÊúÄÈÅ©Âåñ„Åô„Çã„Ç¢„Éó„É™„Åß„Åô„ÄÇÁ∞°ÊΩî„ÅßÂΩπÁ´ã„Å§ÂøúÁ≠î„Çí„Åó„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆË®ÄËëâ„ÇíÁπ∞„ÇäËøî„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇÈáçË¶ÅÔºöÊó•Êú¨Ë™û„Åß„ÅÆ„ÅøÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    }
    
    context = base_context.get(language, base_context["fr"])
    
    return f"""{context}

Message de l'utilisateur: "{message}"
Instructions importantes:
- NE R√âP√àTE PAS le message de l'utilisateur
- Sois concis et utile
- R√âPONDS UNIQUEMENT DANS LA LANGUE: {language}
- Context: assistant concierge Baguette & M√©tro

R√âPONSE EN {language.upper()}:"""

def build_openrouter_prompt(message: str, language: str) -> str:
    """Construit un prompt intelligent pour OpenRouter"""
    base_context = {
        "fr": "Tu es un assistant concierge expert pour 'Baguette & M√©tro', une application qui optimise les trajets RATP avec arr√™ts boulangerie. Sois concis, utile et √©vite de r√©p√©ter ce que dit l'utilisateur. IMPORTANT: R√©ponds UNIQUEMENT en fran√ßais.",
        "en": "You are a concierge assistant for 'Baguette & M√©tro', an app that optimizes RATP routes with bakery stops. Be concise, helpful and avoid repeating the user's words. IMPORTANT: Respond ONLY in English.",
        "ja": "„ÅÇ„Å™„Åü„ÅØ„Äå„Éê„Ç≤„ÉÉ„Éà„Éª„É°„Éà„É≠„Äç„ÅÆ„Ç≥„É≥„Ç∑„Çß„É´„Ç∏„É•„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇRATP„ÅÆË∑ØÁ∑ö„Çí„Éë„É≥Â±ãÂØÑ„ÇäÈÅì„ÅßÊúÄÈÅ©Âåñ„Åô„Çã„Ç¢„Éó„É™„Åß„Åô„ÄÇÁ∞°ÊΩî„ÅßÂΩπÁ´ã„Å§ÂøúÁ≠î„Çí„Åó„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆË®ÄËëâ„ÇíÁπ∞„ÇäËøî„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇÈáçË¶ÅÔºöÊó•Êú¨Ë™û„Åß„ÅÆ„ÅøÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
    }
    
    context = base_context.get(language, base_context["fr"])
    
    return f"""{context}

Message de l'utilisateur: "{message}"
Instructions importantes:
- NE R√âP√àTE PAS le message de l'utilisateur
- Sois concis et utile
- R√âPONDS UNIQUEMENT DANS LA LANGUE: {language}
- Context: assistant concierge Baguette & M√©tro

R√âPONSE EN {language.upper()}:"""

def get_fallback_response(context: str, language: str) -> str:
    """Fallback simple pour messages vides"""
    fallbacks = {
        "fr": "Comment puis-je vous aider aujourd'hui ?",
        "en": "How can I help you today?",
        "ja": "‰ªäÊó•„ÅØ„Å©„ÅÆ„Çà„ÅÜ„Å™„ÅîÁî®‰ª∂„Åß„Åô„ÅãÔºü"
    }
    return fallbacks.get(language, fallbacks["fr"])

def get_intelligent_fallback(message: str, language: str) -> str:
    """Fallback intelligent bas√© sur le contexte du message"""
    message_lower = message.lower()
    
    # D√©tection du contexte et r√©ponse intelligente
    if any(word in message_lower for word in ['comment', 'how', '„Å©„ÅÜ', 'fonctionne', 'work']):
        return get_context_response("how_it_works", language)
    elif any(word in message_lower for word in ['boulangerie', 'bakery', '„Éë„É≥Â±ã', 'pain', 'croissant']):
        return get_context_response("bakery", language)
    elif any(word in message_lower for word in ['m√©tro', 'metro', '„É°„Éà„É≠', 'transport', 'bus', 'ratp']):
        return get_context_response("metro", language)
    elif any(word in message_lower for word in ['paris', 'visiter', 'tourisme', 'voyage', 'visit', 'tourism', 'first time']):
        return get_context_response("paris_tourism", language)
    elif any(word in message_lower for word in ['bonjour', 'salut', 'hello', 'hi', '„Åì„Çì„Å´„Å°„ÅØ']):
        return get_context_response("greeting", language)
    else:
        return get_context_response("default", language)

def get_context_response(context: str, language: str) -> str:
    """Retourne la r√©ponse contextuelle dans la langue appropri√©e"""
    responses = {
        "fr": {
            "how_it_works": "ü•ñ Baguette & M√©tro fonctionne ainsi : 1) Entrez votre d√©part et destination, 2) Cliquez sur 'Calculer l'itin√©raire', 3) L'IA trouve le meilleur trajet avec des boulangeries sur le chemin ! üöá",
            "bakery": "ü•ñ Les boulangeries parisiennes sont excellentes ! Notre app trouve automatiquement les meilleures sur votre trajet. Essayez de calculer un itin√©raire pour voir ! üó∫Ô∏è",
            "metro": "üöá Le m√©tro parisien est tr√®s pratique ! Notre app utilise l'API PRIM RATP pour des donn√©es en temps r√©el. Calculons ensemble votre trajet optimal ! üöÄ",
            "paris_tourism": "üèõÔ∏è Paris est magnifique ! Notre app vous aide √† planifier vos d√©placements et d√©couvrir les meilleures boulangeries. Voulez-vous calculer un itin√©raire ? ü•ñ",
            "greeting": "Bonjour ! üëã Je suis votre assistant Baguette & M√©tro. Je peux vous aider avec les transports parisiens et les boulangeries. Que souhaitez-vous savoir ? üöáü•ñ",
            "default": "ü§ñ Je peux vous aider avec les transports parisiens, les boulangeries et la planification d'itin√©raires. Posez-moi une question sp√©cifique ! üöáü•ñ"
        },
        "en": {
            "how_it_works": "ü•ñ Baguette & M√©tro works like this: 1) Enter your departure and destination, 2) Click 'Calculate Route', 3) AI finds the best route with bakeries along the way! üöá",
            "bakery": "ü•ñ Parisian bakeries are excellent! Our app automatically finds the best ones on your route. Try calculating a route to see! üó∫Ô∏è",
            "metro": "üöá The Paris metro is very convenient! Our app uses the PRIM RATP API for real-time data. Let's calculate your optimal route together! üöÄ",
            "paris_tourism": "üèõÔ∏è Paris is beautiful! Our app helps you plan your trips and discover the best bakeries. Would you like to calculate a route? ü•ñ",
            "greeting": "Hello! üëã I'm your Baguette & M√©tro assistant. I can help you with Parisian transport and bakeries. What would you like to know? üöáü•ñ",
            "default": "ü§ñ I can help you with Parisian transport, bakeries and route planning. Ask me a specific question! üöáü•ñ"
        },
        "ja": {
            "how_it_works": "ü•ñ „Éê„Ç≤„ÉÉ„Éà„Éª„É°„Éà„É≠„ÅØ„Åì„ÅÆ„Çà„ÅÜ„Å´Âãï‰Ωú„Åó„Åæ„ÅôÔºö1) Âá∫Áô∫Âú∞„Å®ÁõÆÁöÑÂú∞„ÇíÂÖ•Âäõ„ÄÅ2) „Äå„É´„Éº„ÉàË®àÁÆó„Äç„Çí„ÇØ„É™„ÉÉ„ÇØ„ÄÅ3) AI„Åå„Éë„É≥Â±ã„Åï„ÇìÂØÑ„ÇäÈÅì„ÅÆÊúÄÈÅ©„É´„Éº„Éà„ÇíË¶ã„Å§„Åë„Åæ„ÅôÔºÅüöá",
            "bakery": "ü•ñ „Éë„É™„ÅÆ„Éë„É≥Â±ã„Åï„Çì„ÅØÁ¥†Êô¥„Çâ„Åó„ÅÑ„Åß„ÅôÔºÅÁßÅ„Åü„Å°„ÅÆ„Ç¢„Éó„É™„ÅåËá™ÂãïÁöÑ„Å´„É´„Éº„Éà‰∏ä„ÅÆÊúÄÈ´ò„ÅÆ„Éë„É≥Â±ã„Åï„Çì„ÇíË¶ã„Å§„Åë„Åæ„Åô„ÄÇ„É´„Éº„Éà„ÇíË®àÁÆó„Åó„Å¶„Åø„Å¶„Åè„Å†„Åï„ÅÑÔºÅüó∫Ô∏è",
            "metro": "üöá „Éë„É™„ÅÆ„É°„Éà„É≠„ÅØ„Å®„Å¶„ÇÇ‰æøÂà©„Åß„ÅôÔºÅÁßÅ„Åü„Å°„ÅÆ„Ç¢„Éó„É™„ÅØPRIM RATP API„Çí‰ΩøÁî®„Åó„Å¶„É™„Ç¢„É´„Çø„Ç§„É†„Éá„Éº„Çø„ÇíÂèñÂæó„Åó„Åæ„Åô„ÄÇ‰∏ÄÁ∑í„Å´ÊúÄÈÅ©„Å™„É´„Éº„Éà„ÇíË®àÁÆó„Åó„Åæ„Åó„Çá„ÅÜÔºÅüöÄ",
            "paris_tourism": "üèõÔ∏è „Éë„É™„ÅØÁæé„Åó„ÅÑË°ó„Åß„ÅôÔºÅÁßÅ„Åü„Å°„ÅÆ„Ç¢„Éó„É™„ÅåÊóÖË°åË®àÁîª„Å®ÊúÄÈ´ò„ÅÆ„Éë„É≥Â±ã„Åï„ÇìÁô∫Ë¶ã„Çí„ÅäÊâã‰ºù„ÅÑ„Åó„Åæ„Åô„ÄÇ„É´„Éº„Éà„ÇíË®àÁÆó„Åó„Åæ„Åô„ÅãÔºüü•ñ",
            "greeting": "„Åì„Çì„Å´„Å°„ÅØÔºÅüëã „Éê„Ç≤„ÉÉ„Éà„Éª„É°„Éà„É≠„ÅÆ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ„Éë„É™„ÅÆ‰∫§ÈÄöÊ©üÈñ¢„Å®„Éë„É≥Â±ã„Åï„Çì„Å´„Å§„ÅÑ„Å¶„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Åæ„Åô„ÄÇ‰Ωï„ÇíÁü•„Çä„Åü„ÅÑ„Åß„Åô„ÅãÔºüüöáü•ñ",
            "default": "ü§ñ „Éë„É™„ÅÆ‰∫§ÈÄöÊ©üÈñ¢„ÄÅ„Éë„É≥Â±ã„Åï„Çì„ÄÅ„É´„Éº„ÉàË®àÁîª„Å´„Å§„ÅÑ„Å¶„ÅäÊâã‰ºù„ÅÑ„Åß„Åç„Åæ„Åô„ÄÇÂÖ∑‰ΩìÁöÑ„Å™Ë≥™Âïè„Çí„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºÅüöáü•ñ"
        }
    }
    
    return responses.get(language, responses["fr"]).get(context, responses["fr"]["default"])

def detect_language_from_message(message: str) -> str:
    """D√©tecte automatiquement la langue de la question"""
    message_lower = message.lower()
    
    # D√©tection anglaise
    english_words = ['what', 'how', 'where', 'when', 'why', 'first', 'time', 'paris', 'visit', 'tour', 'help', 'hello', 'hi']
    if any(word in message_lower for word in english_words):
        return "en"
    
    # D√©tection japonaise
    japanese_chars = ['„ÅØ', '„Åå', '„Çí', '„Å´', '„Å∏', '„Åß', '„ÅÆ', '„Åã', '„Çà', '„Å≠', '„Çè', '„ÅÑ', '„ÅÜ', '„Åà', '„Åä', '„ÅÇ']
    if any(char in message for char in japanese_chars):
        return "ja"
    
    # Par d√©faut : fran√ßais
    return "fr"
